
浮点精度
========

由于数值精度降低，很难保证充分训练网络所需的准确率水平。标准台式计算机上的自动区分方法对于机器精度大约是准确的。以
10^-16 的精度计算导数令人难以置信，但是对 8
位值使用自动微分将会导致较差的结果。在反向传播过程中，这些导数会组合并最终用于更新神经参数。在如此低的数值精度下，模型的准确度可能很差。\ `1 <https://www.jiqizhixin.com/articles/2020-11-02-7>`__

话虽如此，神经网络已经使用 16 位和 8 位浮点数进行了训练。

2015 年，Suyog Gupta
及其同事发表的第一篇有关降低深度学习中的数值精度的论文是《Deep Learning
with Limited Numerical Precision》。论文结果表明，32
位浮点表示形式可以减少为 16
位定点表示形式，而准确度几乎没有降低。但这是使用 stochastic rounding
算法的唯一情况， 因为通常来说，它会产生无偏结果。

2018 年，Naigang Wang 及其同事在其论文《Training Deep Neural Networks
with 8-bit Floating Point Numbers》中使用 8
位浮点数训练了神经网络。由于需要在反向传播期间保持梯度计算的保真度（在使用自动微分时能够实现机器精度），因此使用
8 位数字来训练神经网络要比使用推理更有挑战性。
