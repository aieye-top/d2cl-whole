
卷积核压缩
==========

较小卷积核的参数量和计算成本远小于更大的卷积核。然而，删除掉所有的大型卷积层意味着影响
DNN
的平移不变性，会降低准确度。有些研究通过识别冗余的卷积核并替换为较小的卷积核。例如
SqueezeNet 就通过三种策略用 11 的卷积核替换 33 的卷积核来减小参数。

表 3 对比了不同卷积核压缩技术的性能：相比于 AlexNet，SqueezeNet
的参数减少了 98%，但增加了大量的操作；MobileNet 在瓶颈层（bottleneck
layers）使用深度可分离卷积减小计算量、延迟和参数量；在使用深度可分离卷积时，通过保留较小的特征，只扩展到较大的特征空间，实现了较高的精度。

矩阵分解
--------

张量分解和矩阵分解以和-积的形式加速 DNN
操作，通过将多维张量分解为更小的多个矩阵来消除冗余计算。一些因式分解方法能够将
DDNs
提速至四倍，因为它们创建了更密集的参数矩阵，能够避免非结构化稀疏乘法的局部问题。为了最小化精度损失，每次只实现单层的矩阵分解。首先对一层的参数进行分解，随后根据重构误差来分解后续层。这种按层分解的方法难以应用到大型的
DNNs 中，因为随着 DNN
深度的增加，因式分解的超参数将会呈指数级增长。为了将这种方法在大型 DNNs
中实现，Wen 等人使用了紧凑的核形状和深层结构来减小分解超参数的数量。

因式分解的方法众多，大多数都可以应用于 DNN
的加速，但有些方法无法在精度和计算复杂度之间实现最佳的平衡。例如，典型聚并分解（Canonical
Polyadic
Decompo\ `1 <https://www.infoq.cn/article/Y8UuJHRTGvrZVKKPJFh2?utm_source=related_read_bottom&utm_medium=article>`__\ sition,
CPD）和批量归一化分解（Batch Normalization Decomposition,
BMD）在精度上能够取得很好的表现，但 Tucker-2
分解和奇异值分解的精度就较差。典型聚并分解比批量归一化分解具有更好的压缩效果，但是批量归一化分解取得的准确性要优于典型聚并分解。除此以外，典型聚并分解的优化问题有时并不可解，而批量归一化分解的解却始终存在。

这种技术的优势在于，卷积层和全连接层可以使用相同的矩阵分解方法，通过 CPD
和 BMD
均能够取得不错的性能。但由于理论理解的限制，难以说明为什么有些分解方法的效果能够达到较好的精度，有些方法却不能。此外，由于矩阵分解造成及计算开销常常与减小操作量获得的性能抵消。同时随着
DNNs
深度的增加，应用矩阵分解的训练时间整体会呈指数级增长，因此很难应用于大型网络。而这是由于在空间内搜索以寻找正确分解超参数的时间过大，可以在训练过程中学习超时参数，而非在整个空间内进行搜索，从而加速对大型
DNN 的训练。
