<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>3.1. 模型压缩 &#8212; Dive into cheap deep learning 0.0.2 documentation</title>

    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.2. 参数剪枝(Pruning)" href="pruning.html" />
    <link rel="prev" title="3. Compression" href="index.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">3. </span>Compression</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">3.1. </span>模型压缩</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_compression/compression.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://aieye-top.github.io/d2cl/d2cl.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PDF
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/aieye-top/d2cl">
                  <i class="fab fa-github"></i>
                  Github
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Dive into cheap deep learning
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/time.html">1.1. time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/technology.html">1.2. 技术</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/privacy.html">1.3. 隐私</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/money.html">1.4. money</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/data.html">1.5. Data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_lightweight/index.html">2. Lightweight</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/lightweight.html">2.1. Lightweight</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/squeezenet.html">2.2. SqueezeNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/mobilenet.html">2.3. MobileNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/mobilenet_v2.html">2.4. MobileNet-v2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/shufflenet.html">2.5. ShuffleNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/GhostNet.html">2.6. GhostNet</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. Compression</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.1. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="pruning.html">3.2. 参数剪枝(Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Knowledge-Distillation.html">3.3. Knowledge-Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization.html">3.4. 量化</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_write_code/index.html">4. Write code</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_write_code/jupyter.html">4.1. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_write_code/API.html">4.2. API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_train/index.html">5. Train</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/Server.html">5.1. Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/Active_Learning.html">5.2. Active Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/pretrain.html">5.3. Pretrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/improve.html">5.4. 改进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/structure.html">5.5. 结构</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deploy/index.html">6. Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/hardware.html">6.1. 芯片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/edge.html">6.2. Edge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/mobile.html">6.3. mobile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/MCU.html">6.4. MCU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/AI-zhongtai.html">6.5. AI 中台</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Dive into cheap deep learning
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/time.html">1.1. time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/technology.html">1.2. 技术</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/privacy.html">1.3. 隐私</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/money.html">1.4. money</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/data.html">1.5. Data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_lightweight/index.html">2. Lightweight</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/lightweight.html">2.1. Lightweight</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/squeezenet.html">2.2. SqueezeNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/mobilenet.html">2.3. MobileNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/mobilenet_v2.html">2.4. MobileNet-v2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/shufflenet.html">2.5. ShuffleNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_lightweight/GhostNet.html">2.6. GhostNet</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. Compression</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.1. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="pruning.html">3.2. 参数剪枝(Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Knowledge-Distillation.html">3.3. Knowledge-Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization.html">3.4. 量化</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_write_code/index.html">4. Write code</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_write_code/jupyter.html">4.1. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_write_code/API.html">4.2. API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_train/index.html">5. Train</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/Server.html">5.1. Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/Active_Learning.html">5.2. Active Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/pretrain.html">5.3. Pretrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/improve.html">5.4. 改进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_train/structure.html">5.5. 结构</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deploy/index.html">6. Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/hardware.html">6.1. 芯片</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/edge.html">6.2. Edge</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/mobile.html">6.3. mobile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/MCU.html">6.4. MCU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deploy/AI-zhongtai.html">6.5. AI 中台</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">3.1. </span>模型压缩<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2><span class="section-number">3.1.1. </span>为什么需要模型压缩和加速<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>预训练后的深度神经网络模型往往存在着严重的 过参数化
问题，其中只有约5%的参数子集是真正有用的。为此，对模型进行 时间 和 空间
上的压缩，便谓之曰“模型压缩” 。</p>
<p>模型压缩技术的核心是确定每个层的压缩策略，因为它们具有不同的冗余，这通常需要手工试验和领域专业知识来探索模型大小、速度和准确性之间的大设计空间。这个设计空间非常大，人工探索法通常是次优的，而且手动进行模型压缩非常耗时。<a class="reference external" href="https://blog.csdn.net/weixin_34144848/article/details/89662408?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-18.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-18.controls">4</a></p>
<p>模型压缩技术包括 前端压缩 和 后端压缩 这两部分。</p>
<p>复杂网络的实时性需求催生了另一个工作方向，这个工作方向的主要目的是研究如何将科研人员的研究成果落地到实际的硬件与应用场景中，并且要确保算法稳定、高效，从事相关的工作的人员就是所谓的工程派。但是我们知道复杂网络的参数数量肯定远多于简单网络的参数数量，这就导致复杂网络的模型体积远大于简单网络的模型体积，很多模型的体积极易超过1GB，这种体积的模型无论是在移动平台上存储（存储空间不足）还是执行（内存更小）都极其困难，因此工程派面临的第一个问题就是在尽量不降低模型精度的情况下缩小模型的体积。</p>
<p>在模型压缩领域目前最重要的几个方法都是由Song Han在其文章《Deep
Compression》[插图]中发表的，这篇论文是ICLR2016的最佳论文，它让工程人员看到了深度神经网络在移动平台设备落地的可能性，引导了后面对模型压缩的一系列研究。不过其实早在1989年，Lecun就已提出了OBD（Optimal
Brain
Damage）这种剔除模型中不重要的参数以减小网络体积的方法，只不过当时的计算资源还不足以解决神经网络的计算，更不用说深度神经网络了，即便如此该方法仍具有很好的前瞻性，目前很多方法基本都是基于该方法的思路。</p>
<p>目前深度学习模型压缩方法的研究主要可以分为以下几个方向。<a class="reference external" href="https://weread.qq.com/web/reader/5a5326d0719ecf5f5a52e7ek0723244023c072b030ba601">2</a>
1.
更精细模型的设计。目前很多网络都具有模块化设计，在深度和宽度上都很大，这也造成了参数的冗余，因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，都使用更加细致、高效的模型设计，能够很大程度地缩小模型尺寸，并且也具有不错的性能。这些相关内容已经在前面的章节介绍过，此处不再赘述。
2.
权重稀疏化。在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。
3.
模型裁剪。结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的连接或者过滤器进行裁剪以减少模型的冗余。</p>
</div>
<div class="section" id="the-literature-classification-and-quantity-of-the-review">
<h2><span class="section-number">3.1.2. </span>The literature classification and quantity of the review<a class="headerlink" href="#the-literature-classification-and-quantity-of-the-review" title="Permalink to this headline">¶</a></h2>
<p>根据图2所示的文章发表年份来看,文献[8~11]的最新文章发表于2017年,对近年热门研究方向和新方法的介绍较少.而根据我们的最新整理,2018年之后发表在各大顶级会议的文章达64篇,占本文统计文章总数约40%,其中[13]首先提出在裁剪权重时加入能耗、延迟等硬件限制作为优化约束,为后续工作[14,15,16]提供启发.Network
Trimming[17]将激活值为0的通道数量作为判断filter是否重要的标准,是结构化剪枝领域最有影响力的工作,开创了设置filter评价因子的技术分支.[18]提出的依据参数对应损失函数(loss)的梯度来自适应确定每个参数量化位数的方法,打破了固有的手工确定量化位数的观念,引领了新的自适应量化技术体系.由此看出近年出现的热门文章提供了不少新的研究方向,极大促进模型压缩与加速领域的发展,非常值得收录到我们的综述中,为读者带来新的思考.</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">3.1.3. </span>14.3 模型压缩方法 411<a class="reference external" href="https://furui&#64;phei.com.cn/module/goods/wssd_content.jsp?bookid=57454">3</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>前端压缩和后端压缩对比 411 网络剪枝 411 典型剪枝方法的对比 413 网络蒸馏
413 前端压缩 413 后端压缩 414 低秩分解 416 总体压缩效果评价指标 416</p>
<div class="section" id="id4">
<h3><span class="section-number">3.1.3.1. </span>前端压缩<a class="reference external" href="https://leesen998.github.io/2018/01/15/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/">5</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>（1）知识蒸馏（简单介绍）
一个复杂的模型可以认为是由多个简单模型或者强约束条件训练而来，具有很好的性能，但是参数量很大，计算效率低，而小模型计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。但与剪枝、量化等方法想比，效果较差。(<a class="reference external" href="https://blog.csdn.net/Lucifer_zzq/article/details/79489248">https://blog.csdn.net/Lucifer_zzq/article/details/79489248</a>)
（2）紧凑的模型结构设计（简单介绍）
紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。
（3）滤波器层面的剪枝（简单介绍） 参考链接
<a class="reference external" href="https://blog.csdn.net/JNingWei/article/details/79218745">https://blog.csdn.net/JNingWei/article/details/79218745</a> 补充优缺点
滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。</p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">3.1.3.2. </span>后端压缩<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id6">
<h4><span class="section-number">3.1.3.2.1. </span>低秩近似 （简单介绍，参考链接补充优缺点）<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。</p>
<p>优点：</p>
<ol class="arabic simple">
<li><p>可以降低存储和计算消耗；</p></li>
<li><p>一般可以压缩2-3倍；精度几乎没有损失；</p></li>
</ol>
<p>缺点：</p>
<p>模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能</p>
</div>
<div class="section" id="id7">
<h4><span class="section-number">3.1.3.2.2. </span>未加限制的剪枝 （简单介绍，参考链接补充优缺点）<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>剪枝操作包括：非结构化剪枝和结构化剪枝。非结构化剪枝是对神经网络中权重较小的权重或者权重矩阵进剔除，然后对整个神经网络进行微调；结构化剪枝是在网络优化目标中加入权重稀疏正则项，使部分权重在训练时趋于0。</p>
<p>优点：</p>
<ol class="arabic simple">
<li><p>保持模型性能不损失的情况下，减少参数量9-11倍；</p></li>
<li><p>剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力；</p></li>
</ol>
<p>缺点：</p>
<ol class="arabic simple">
<li><p>非结构化剪枝会增加内存访问成本；</p></li>
<li><p>极度依赖专门的运行库和特殊的运行平台，不具有通用性；</p></li>
</ol>
<p>压缩率过大时，破坏性能；</p>
</div>
<div class="section" id="id8">
<h4><span class="section-number">3.1.3.2.3. </span>参数量化 （简单介绍，参考链接补充优缺点）<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享
优点：</p>
<p>模型性能损失很小，大小减少8-16倍； 缺点：</p>
<p>压缩率大时，性能显著下降；</p>
<p>依赖专门的运行库，通用性较差；</p>
</div>
<div class="section" id="id9">
<h4><span class="section-number">3.1.3.2.4. </span>二值网络 （简单介绍，参考链接补充优缺点）<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>对于32bit浮点型数用1bit二进制数-1或者1表示。 优点：</p>
<p>网络体积小，运算速度快</p>
</div>
</div>
</div>
<div class="section" id="iccv2019">
<h2><span class="section-number">3.1.4. </span>针对生成模型的协同进化压缩算法(ICCV2019)<a class="headerlink" href="#iccv2019" title="Permalink to this headline">¶</a></h2>
<p>在CycleGAN中的两个生成器网络将会被同时压缩：</p>
<div class="math notranslate nohighlight" id="equation-chapter-compression-compression-0">
<span class="eqno">(3.1.1)<a class="headerlink" href="#equation-chapter-compression-compression-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\hat{G}_{1}, \hat{G}_{2} &amp;=\arg \min _{G_{1}, G_{2}} \mathcal{N}\left(G_{1}\right)+\mathcal{N}\left(G_{2}\right) \\
&amp;+\gamma\left(\mathcal{L}_{\text {DisA}}\left(G_{1}, D_{1}\right)+\lambda \mathcal{L}_{\text {cyc }}\left(G_{1}, G_{2}, X\right)\right) \\
\quad+&amp; \gamma\left(\mathcal{L}_{\text {DisA }}\left(G_{2}, D_{2}\right)+\lambda \mathcal{L}_{\text {cyc }}\left(G_{2}, G_{1}, Y\right)\right)
\end{aligned}\end{split}\]</div>
</div>
<div class="section" id="automl-amc">
<h2><span class="section-number">3.1.5. </span>AutoML模型压缩（AMC）<a class="headerlink" href="#automl-amc" title="Permalink to this headline">¶</a></h2>
<p>基于学习而非规则</p>
<p>这些基于规则的剪枝策略并非是最优的，而且不能从一个模型转移到另一个模型。随着神经网络结构的快速发展，我们需要一种自动化的方法来压缩它们，以提高工程师的效率。</p>
<p>我们观察到压缩模型的精度对每层的稀疏性非常敏感，需要细粒度的动作空间。因此，我们不是在一个离散的空间上搜索，而是通过
DDPG agent
提出连续压缩比控制策略，通过反复试验来学习：在精度损失时惩罚，在模型缩小和加速时鼓励。actor-critic
的结构也有助于减少差异，促进更稳定的训练。</p>
<p>针对不同的场景，我们提出了两种压缩策略搜索协议:</p>
<p> ●  对于 latency-critical 的 AI 应用（例如，手机
APP，自动驾驶汽车和广告排名），我们建议采用资源受限的压缩（resource-constrained
compression），在最大硬件资源（例如，FLOP，延迟和模型大小）下实现最佳精度
）；  ●  对于 quality-critical 的 AI 应用（例如 Google
Photos），我们提出精度保证的压缩（accuracy-guaranteed
compression），在实现最小尺寸模型的同时不损失精度。 DDPG Agent</p>
<p> ●  DDPG Agent 用于连续动作空间（0-1）
 ●  输入每层的状态嵌入，输出稀疏比 压缩方法研究</p>
<p> ●  用于模型大小压缩的细粒度剪枝（ Fine-grained Pruning）  ●  粗粒度 /
通道剪枝，以加快推理速度 搜索协议</p>
<p> ●  资源受限压缩，以达到理想的压缩比，同时获得尽可能高的性能。
 ●  精度保证压缩，在保持最小模型尺寸的同时，完全保持原始精度。
为了保证压缩的准确性，我们定义了一个精度和硬件资源的奖励函数。有了这个奖励函数，就能在不损害模型精度的情况下探索压缩的极限。</p>
<p> ●  对于资源受限的压缩，只需使用 Rerr = -Error
 ●  对于精度保证的压缩，要考虑精度和资源（如 FLOPs）：RFLOPs =
-Error∙log（FLOPs）</p>
<p>“算力换算法”是当今AutoML系列工作的热点话题，AMC则属于“算力换算力”：用training时候的算力换取inference时候的算力。模型在完成一次训练之后，可能要在云上或移动端部署成千上万次，所以inference的速度和功耗至关重要。</p>
<p>我们用AutoML做一次性投入来优化模型的硬件效率，然后在inference的时候可以得到事半功倍的效果。比如AMC将MobileNet
inference时的计算量从569M MACs降低到285M
MACs，在Pixel-1手机上的速度由8.1fps提高到14.6fps，仅有0.1%的top-1准确率损失。AMC采用了合适的搜索空间，对压缩策略的搜索仅需要4个GPU
hours。</p>
<p>总结来讲，AMC用“Training算力”换取“Inference算力”的同时减少的对“人力“的依赖。最后，感谢Google
Cloud AI对本项目的支持。</p>
<p><a class="reference external" href="https://github.com/PaddlePaddle/PaddleSlim">https://github.com/PaddlePaddle/PaddleSlim</a></p>
<p>[8] Lei J, Gao X, Song J, Wang XL, Song ML. Survey of deep neural
network model compression. Ruan Jian Xue Bao/Journal of Software,
2018,29(2):251−266 (in Chinese).
<a class="reference external" href="http://www.jos.org.cn/1000-9825/5428.htm">http://www.jos.org.cn/1000-9825/5428.htm</a> [9] Ji RZ,Lin SH,Chao F,Wu
YJ,Huang FY. Deep neural network compression and acceleration. Computer
research and development,2018,55(09):1871-1888(in
Chinese).http://crad.ict.ac.cn/CN/10.7544/issn1000-1239.2018.20180129
[10] Cao WL,Rui JW,Li M. Survey of neural network model compression
methods.Computer application research,2019,36(03):649-656(in Chinese).
[11] Cheng Y, Wang D, Zhou P, et al. A survey of model compression and
acceleration for deep neural networks. arXiv preprint arXiv:1710.09282,
2017. [12] Cheng J, Wang P, Li G, et al. Recent advances in efficient
computation of deep convolutional neural networks. Frontiers of
Information Technology &amp; Electronic Engineering, 2018, 19(1): 64-77.
[13] Chen C, Tung F, Vedula N, et al. Constraint-aware deep neural
network compression. Proceedings of the European Conference on Computer
Vision (ECCV). 2018: 400-415. [14] Yang H, Zhu Y, Liu J.
Energy-constrained compression for deep neural networks via weighted
sparse projection and layer input masking. arXiv preprint
arXiv:1806.04321, 2018. [15] Yang T J, Chen Y H, Sze V. Designing
energy-efficient convolutional neural networks using energy-aware
pruning. Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 2017: 5687-5695. [16] Yang H, Zhu Y, Liu J. Ecc:
Platform-independent energy-constrained deep neural network compression
via a bilinear regression model. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 2019: 11206-11215. [17] Hu H,
Peng R, Tai Y W, et al. Network trimming: A data-driven neuron pruning
approach towards efficient deep architectures. arXiv preprint
arXiv:1607.03250, 2016. [18] Khoram S, Li J. Adaptive quantization of
neural networks. 2018</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">3.1. 模型压缩</a><ul>
<li><a class="reference internal" href="#id2">3.1.1. 为什么需要模型压缩和加速</a></li>
<li><a class="reference internal" href="#the-literature-classification-and-quantity-of-the-review">3.1.2. The literature classification and quantity of the review</a></li>
<li><a class="reference internal" href="#id3">3.1.3. 14.3 模型压缩方法 4113</a><ul>
<li><a class="reference internal" href="#id4">3.1.3.1. 前端压缩5</a></li>
<li><a class="reference internal" href="#id5">3.1.3.2. 后端压缩</a><ul>
<li><a class="reference internal" href="#id6">3.1.3.2.1. 低秩近似 （简单介绍，参考链接补充优缺点）</a></li>
<li><a class="reference internal" href="#id7">3.1.3.2.2. 未加限制的剪枝 （简单介绍，参考链接补充优缺点）</a></li>
<li><a class="reference internal" href="#id8">3.1.3.2.3. 参数量化 （简单介绍，参考链接补充优缺点）</a></li>
<li><a class="reference internal" href="#id9">3.1.3.2.4. 二值网络 （简单介绍，参考链接补充优缺点）</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#iccv2019">3.1.4. 针对生成模型的协同进化压缩算法(ICCV2019)</a></li>
<li><a class="reference internal" href="#automl-amc">3.1.5. AutoML模型压缩（AMC）</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>3. Compression</div>
         </div>
     </a>
     <a id="button-next" href="pruning.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>3.2. 参数剪枝(Pruning)</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>