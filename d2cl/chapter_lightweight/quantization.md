# 量化

## 定义

参数量化是指用较低位宽表示典型的32位浮点网络参数,网络参数包括权重、激活值、梯度和误差等等,可以使用统一的位宽(如16-bit、8-bit、2-bit和1-bit等),也可以根据经验或一定策略自由组合不同的位宽.

### 优点

(1) 能够显著减少参数存储空间与内存占用空间,将参数从32位浮点型量化到8位整型,从而缩小75%的存储空间,这对于计算资源有限的边缘设备和嵌入式设备进行深度学习模型的部署和使用都有很大的帮助;
(2) 能够加快运算速度,降低设备能耗,读取32位浮点数所需的带宽可以同时读入4个8位整数,并且整型运算相比浮点型运算更快,自然能够降低设备功耗.但其仍存在一定的局限性,网络参数的位宽减少损失了一部分信息量,会造成推理精度的下降,虽然能够通过微调恢复部分精确度,但也带来时间成本的增加;量化到特殊位宽时,很多现有的训练方法和硬件平台不再适用,需要设计专用的系统架构,灵活性不高.

量化就是将神经网络的浮点算法转换为定点。这样就可以在移动手机上实现网络的实时运算，对云计算的部署也有帮助。[2]

常规精度一般使用 FP32（32位浮点，单精度）存储模型权重；低精度则表示 FP16（半精度浮点），INT8（8位的定点整数）等等数值格式。不过目前低精度往往指代 INT8。

由于 INT8 使用的比特数只有 FP32 的 25% ，所以如果量化方法得当，我们可以得到精度效果好，但是内存占用大大减少的模型。

## 为什么需要量化？

想象一下，一个使用 ATmega328P 微控制器的 Arduino Uno，它使用 8 位运算。要想在 Uno 上运行一个模型，理想情况下模型权重必须存储为 8 位整数（而许多台式计算机和笔记本电脑使用 32 位或 64 位浮点表示）。通过量化模型，权重的存储大小减少为原来的 4 分之一（如 32 位到 8 位值的量化），而对准确度的影响可以忽略（通常约为 1–3%）。



8 位编码过程中量化误差的示意图（图源：https://tinymlbook.com/）

此外，在量化过程中，由于量化误差，一些信息可能会丢失。为了解决这个问题，量化感知（QA）训练被提出并作为一种替代方案。

## TPU[3]

TPU的高性能还来源于对于低运算精度的容忍。研究结果表明，低精度运算带来的算法准确率损失很小，但是在硬件实现上却可以带来巨大的便利，包括功耗更低、速度更快、占芯片面积更小的运算单元、更小的内存带宽需求等。TPU采用了8比特的低精度运算。

[1]: https://www.jiqizhixin.com/articles/2020-11-02-7
[2]: https://blog.csdn.net/Rocky6688/article/details/107954339as
[3]: https://blog.csdn.net/Rocky6688/article/details/107252916
