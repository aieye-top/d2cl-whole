# Lightweight

移动／嵌入式端的计算能力往往只有桌面级 GPU 的 1/100 到 1/1000

换句话说在 GPU 上一秒 40 帧的深度学习 CV 算法在移动／嵌入式端一秒只有 0.04-0.4 帧，这样的性能会极大影响用户体验。

除了运行速度之外，能效比（energy efficiency）也是关键指标。

因此模型的运算量和模型尺寸大小都是越小越好。我们可以训练新的网络拓扑以减小运算量，也可以使用网络压缩的办法改善运行性能，或者同时使用这两种办法。[1]

## 怎样才算？

深度学习模型大小主要决定的是该模型做一次 inference 所需要的能量。

深度学习模型必须储存在内存里面，而内存其实还分为片上内存和片外内存两种。

- 片上内存就是 SRAM cache，是处理器集成在芯片上用来快速存取重要数据的内存模块。片上内存会占据宝贵的芯片面积，因此处理器中集成的片上内存大小通常在 1-10 MB 这个数量级。
- 片外内存则是主板上的 DDR 内存，这种内存可以做到容量很大（>1 GB），但是其访问速度较慢。

访问片外内存所需要的能量是巨大的。根据 Song Han 在论文中的估计，一次片外内存访问消耗的能量是一次乘-加法运算的 200 倍，同时也是一次访问片上内存所需能量的 128 倍。换句话说，一次片外内存访问相当于做 200 次乘法运算！

## 方法

### 大幅调整模型结构

1. 先驱Bengio 的Binarized Neural Network（将神经元 activation 限制为-1 或 1）
2. Google的MobileNet计算量仅为 1GOP 上下，而模型大小只有 4MB 多一些，但能在 ImageNet 上实现 90% 左右的 top-5 准确率。（同时不同特征之间的权重参数变成线性相关。理论上减小了自由度，但是由于深度学习网络本身就存在冗余，因此实际测试中性能并没有降低很多。）
3. Face++也发表了 ShuffleNet

### 模型压缩

压缩网络的前提是神经网络中的节点是有冗余的


在数据编码上:

1. 降低精度（ 32-bit 浮点数计算换成 16-bit 浮点数甚至 8-bit 定点数）
2. 优化编码（原本的定点数是线性编码数字之间的间距相等，但是可以使用非线性编码在数字集中的地方使数字间的间距变小增加精度，而在数字较稀疏的地方使数字间距较大。效果：8-bit 非线性编码在合适的场合可以达到接近 16-bit 线性编码的精度

知识蒸馏 Knowledge Distillation(KD)

在一开始training的过程中比较庞杂，但是当后来需要拿去deploy的时候，可以转换成一个更小的模型。



### 减少特征图

- 正则化（regularization）：修改目标函数/学习问题，所以优化后可能会得到一个参数较少的神经网络。参见 Louizos et al, (2018)
- 网络修剪（network pruning）：拿掉不活跃的神经元
- 增长（growing）：虽然这一方法传播得不够广泛，但是也可以采取这第三种方法，从小型网络开始，按某种增长标准逐步增加新的单元。参见「学界 | 为数据集自动生成神经网络：普林斯顿大学提出 NeST」

[1]: https://mp.weixin.qq.com/s/lO2UM04PfSM5VJYh6vINhw
[2]: https://antkillerfarm.github.io/dl%20acceleration/2019/07/26/DL_acceleration_5.html
[3]: https://www.zhihu.com/question/50519680/answer/136406661
